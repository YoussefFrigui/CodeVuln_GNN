# File and Directory Paths
data:
  # Path to processed advisories with actual vulnerable code extracted from GitHub commits
  # This file is created by running: python run_pipeline.py --step preprocess
  advisories_path: "outputs/datasets/processed_advisories_with_code.json"
  codesearchnet_dir: "data/python/python/final/jsonl/train"
  processed_dataset_path: "outputs/datasets/final_graph_dataset.pt"
  
  # High-quality curated datasets (new!)
  curated_dataset_path: "outputs/datasets/curated_vulnerabilities.json"
  validated_dataset_path: "outputs/datasets/validated_vulnerabilities.json"
  
  # Data source configuration
  sources:
    use_curated_vulnerabilities: true  # Use high-quality curated data (recommended)
    github_advisories: true
    nvd: false  # Set to true and provide API key
    synthetic: true
    codesearchnet: true

# Data Processing Parameters
dataset:
  # Reduce to balance dataset better with vulnerable examples
  # Current ratio with 36,230: 1:5 (vulnerable:safe)
  # This helps model learn vulnerable patterns better than 1:100 ratio
  max_safe_examples: 20450  #  20450 (1:5)
  max_nodes_per_graph: 100
  
  # Data quality settings
  quality:
    min_quality_score: 0.4  # Minimum quality score (0-1)
    min_loc: 5  # Minimum lines of code
    max_loc: 500  # Maximum lines of code
    require_context: false  # Require full function context
    validate_syntax: true  # Check Python syntax validity
    
  # Diversity settings
  diversity:
    max_per_cwe: 150  # Maximum examples per CWE type
    balance_by_complexity: true  # Balance across complexity levels
    include_synthetic: true  # Include synthetic vulnerabilities
    synthetic_count: 200  # Number of synthetic examples to generate

# Model Architecture
model:
  num_node_features: 11
  hidden_channels: 128
  num_classes: 2
  dropout: 0.3
  gcn_layers: 5         # Increased from 4 to 5 for deeper pattern learning
  gat_heads: 8          # Number of attention heads
  use_batch_norm: true  # Batch normalization for stability
  use_residual: true    # Residual connections for better gradient flow
  pooling: "multi"      # "multi" = mean+max+sum, "dual" = mean+max

# Training Parameters
training:
  device: "auto"  # "auto" will use "cuda" if available, else "cpu"
  num_epochs: 30        # Increased from 20 for better convergence
  batch_size: 64
  learning_rate: 0.0005 # Reduced from 0.001 for stability with deeper model
  weight_decay: 0.0001  # Increased for better regularization
  patience: 7           # More patience with lower learning rate
  test_split_size: 0.1
  validation_split_size: 0.1 # This is 0.1 of the (1 - test_split_size) data
  random_state: 42
  scheduler: "cosine"   # Learning rate scheduler: "cosine", "step", or "none"

# Evaluation and Output
output:
  model_save_path: "outputs/models/trained_gnn_model.pt"

# MLflow Experiment Tracking
mlflow:
  enabled: true
  experiment_name: "GNN_Vulnerability_Detection"
  tracking_uri: "outputs/mlruns"  # Local directory for MLflow tracking
  run_name_prefix: "gnn_vuln"  # Prefix for run names
  log_models: true  # Whether to log models to MLflow
  log_artifacts: true  # Whether to log confusion matrix and other artifacts
