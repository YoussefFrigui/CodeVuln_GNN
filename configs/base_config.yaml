# File and Directory Paths
data:
  # Path to processed advisories with actual vulnerable code extracted from GitHub commits
  # Options:
  #   - merged_vulnerabilities.json: ALL sources combined (recommended)
  #   - processed_advisories_full_files.json: Full-file extraction only
  #   - processed_advisories_with_code.json: Original diff-based extraction
  advisories_path: "outputs/datasets/merged_vulnerabilities.json"
  
  # Individual source files (used by merge script, not directly by training)
  full_files_path: "outputs/datasets/processed_advisories_full_files.json"
  diff_based_path: "outputs/datasets/processed_advisories_with_code.json"
  description_extracted_path: "outputs/datasets/description_extracted_code.json"
  
  codesearchnet_dir: "data/python/python/final/jsonl/train"
  processed_dataset_path: "outputs/datasets/final_graph_dataset.pt"
  
  # High-quality curated datasets (included in merge)
  curated_dataset_path: "outputs/datasets/curated_vulnerabilities.json"
  validated_dataset_path: "outputs/datasets/validated_vulnerabilities.json"
  
  # Data source configuration
  sources:
    use_curated_vulnerabilities: false  # Already included in merged file
    github_advisories: true
    nvd: false  # Set to true and provide API key
    synthetic: true
    codesearchnet: true

# Data Processing Parameters
dataset:
  # With ~7000 vulnerable examples, use more safe examples for balance
  # Ratio: 1:3 (vulnerable:safe) for better learning
  max_safe_examples: 21000  # ~7000 * 3 = 21000 for 1:3 ratio
  max_nodes_per_graph: 100
  
  # Data quality settings
  quality:
    min_quality_score: 0.4  # Minimum quality score (0-1)
    min_loc: 5  # Minimum lines of code
    max_loc: 500  # Maximum lines of code
    require_context: false  # Require full function context
    validate_syntax: true  # Check Python syntax validity
    
  # Diversity settings
  diversity:
    max_per_cwe: 150  # Maximum examples per CWE type
    balance_by_complexity: true  # Balance across complexity levels
    include_synthetic: true  # Include synthetic vulnerabilities
    synthetic_count: 700  # Increased: Number of synthetic vulnerable examples (100 per category x 7 categories)
    simple_safe_count: 1000  # NEW: Simple safe examples to prevent false positives on basic code

# Model Architecture (v2 - Enhanced for reduced false positives)
model:
  num_node_features: 16  # Enhanced: Was 11, now 16 with semantic features
  hidden_channels: 128
  num_classes: 2
  dropout: 0.35          # Slightly increased for better regularization
  gcn_layers: 4          # Reduced from 5 - dual GAT compensates
  gat_heads: 8           # Number of attention heads (split between local/global)
  use_batch_norm: true   # Batch normalization for stability
  use_residual: true     # Residual connections for better gradient flow
  pooling: "multi"       # "multi" = mean+max+attention (v2)

# Training Parameters (optimized for v2 architecture)
training:
  device: "auto"  # "auto" will use "cuda" if available, else "cpu"
  num_epochs: 25         # More epochs for complex architecture
  batch_size: 64
  learning_rate: 0.0003  # Lower LR for stable training with new components
  weight_decay: 0.0002   # Increased for better regularization
  patience: 10           # More patience - model takes longer to converge
  test_split_size: 0.1
  validation_split_size: 0.1 # This is 0.1 of the (1 - test_split_size) data
  random_state: 42
  scheduler: "cosine"    # Learning rate scheduler: "cosine", "step", or "none"

# Evaluation and Output
output:
  model_save_path: "outputs/models/trained_gnn_model.pt"

# MLflow Experiment Tracking
mlflow:
  enabled: true
  experiment_name: "GNN_Vulnerability_Detection"
  tracking_uri: "outputs/mlruns"  # Local directory for MLflow tracking
  run_name_prefix: "gnn_vuln"  # Prefix for run names
  log_models: true  # Whether to log models to MLflow
  log_artifacts: true  # Whether to log confusion matrix and other artifacts

# LLM Explainability (Google Gemini)
llm:
  enabled: true
  provider: "google"  # Currently only Google Gemini is supported
  model: "gemini-2.5-pro"  # Advanced model with thinking capabilities for better analysis
  # api_key: ""  # Set via GEMINI_API_KEY environment variable (recommended)
  temperature: 0.3  # Lower = more focused explanations
  max_output_tokens: 8192  # Maximum output tokens for comprehensive analysis

# Hybrid Analyzer (GNN + LLM combined detection)
hybrid:
  enabled: true
  gnn_weight: 0.15  # GNN provides initial signal only
  llm_weight: 0.85  # LLM is the authoritative scorer - dominates final score
  temperature: 0.1  # Very low temperature for consistent scoring
  max_output_tokens: 8192  # Maximum output tokens for detailed analysis
